#!/usr/bin/env python3
"""
DART ë°ì´í„° ìºì‹œ ì‹œìŠ¤í…œ
===================

ì‹¤ì œ DART API í˜¸ì¶œ ê²°ê³¼ë¥¼ ìºì‹œí•˜ì—¬ ì¤‘ë³µ ìˆ˜ì§‘ì„ ë°©ì§€í•˜ëŠ” ì‹œìŠ¤í…œ

Author: Korean Airlines Credit Rating Analysis
"""

import os
import json
import pickle
import hashlib
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List
import pandas as pd
import logging

logger = logging.getLogger(__name__)

class DARTDataCache:
    """
    DART API ë°ì´í„° ìºì‹œ ê´€ë¦¬ ì‹œìŠ¤í…œ
    """
    
    def __init__(self, cache_dir: str = "dart_cache", cache_duration_hours: int = 24):
        """
        ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            cache_dir: ìºì‹œ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
            cache_duration_hours: ìºì‹œ ìœ íš¨ ì‹œê°„ (ì‹œê°„ ë‹¨ìœ„)
        """
        self.cache_dir = cache_dir
        self.cache_duration = timedelta(hours=cache_duration_hours)
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        self.metadata_file = os.path.join(self.cache_dir, "cache_metadata.json")
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ ë˜ëŠ” ì´ˆê¸°í™”
        self.metadata = self._load_metadata()
        
        logger.info(f"âœ… DART Cache initialized: {self.cache_dir}")
        logger.info(f"ğŸ• Cache duration: {cache_duration_hours} hours")
    
    def _load_metadata(self) -> Dict[str, Any]:
        """ìºì‹œ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        logger.info(f"ğŸ“– [CACHE] Loading metadata from: {self.metadata_file}")
        
        if os.path.exists(self.metadata_file):
            try:
                logger.info(f"âœ… [CACHE] Metadata file exists, loading...")
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                logger.info(f"âœ… [CACHE] Metadata loaded successfully: {len(metadata.get('entries', {}))} entries")
                return metadata
            except json.JSONDecodeError as e:
                logger.error(f"âŒ [CACHE] JSON decode error loading metadata: {e}")
                logger.error(f"ğŸ“ [CACHE] Corrupted metadata file: {self.metadata_file}")
            except Exception as e:
                logger.error(f"âŒ [CACHE] Failed to load cache metadata: {e}")
                import traceback
                logger.error(f"âŒ [CACHE] Traceback: {traceback.format_exc()}")
        else:
            logger.info(f"ğŸ“ [CACHE] Metadata file not found, creating new metadata")
        
        # ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° ìƒì„±
        new_metadata = {
            "cache_created": datetime.now().isoformat(),
            "total_entries": 0,
            "entries": {}
        }
        logger.info(f"âœ… [CACHE] Created new metadata")
        return new_metadata
    
    def _save_metadata(self):
        """ìºì‹œ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        try:
            logger.debug(f"ğŸ’¾ [CACHE] Saving metadata to: {self.metadata_file}")
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
            logger.debug(f"âœ… [CACHE] Metadata saved successfully")
        except Exception as e:
            logger.error(f"âŒ [CACHE] Failed to save cache metadata: {e}")
            import traceback
            logger.error(f"âŒ [CACHE] Traceback: {traceback.format_exc()}")
    
    def _generate_cache_key(self, corp_code: str, year: int, quarter: int, data_type: str = "financial") -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_string = f"{corp_code}_{year}_{quarter}_{data_type}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _get_cache_file_path(self, cache_key: str) -> str:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
        return os.path.join(self.cache_dir, f"{cache_key}.pkl")
    
    def is_cache_valid(self, cache_key: str) -> bool:
        """ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            logger.debug(f"ğŸ” [CACHE] Checking validity for key: {cache_key}")
            
            if cache_key not in self.metadata["entries"]:
                logger.debug(f"âŒ [CACHE] Key not found in metadata: {cache_key}")
                return False
            
            entry = self.metadata["entries"][cache_key]
            cached_time = datetime.fromisoformat(entry["cached_at"])
            current_time = datetime.now()
            age = current_time - cached_time
            
            logger.debug(f"ğŸ• [CACHE] Cache age: {age}, duration limit: {self.cache_duration}")
            
            is_valid = age < self.cache_duration
            logger.debug(f"{'âœ…' if is_valid else 'âŒ'} [CACHE] Cache validity: {is_valid}")
            
            return is_valid
            
        except Exception as e:
            logger.error(f"âŒ [CACHE] Error checking cache validity: {e}")
            return False
    
    def get_cached_data(self, corp_code: str, year: int, quarter: int, data_type: str = "financial") -> Optional[Any]:
        """ìºì‹œëœ ë°ì´í„° ì¡°íšŒ"""
        try:
            logger.info(f"ğŸ” [CACHE] get_cached_data called: {corp_code} {year}Q{quarter} ({data_type})")
            
            cache_key = self._generate_cache_key(corp_code, year, quarter, data_type)
            logger.info(f"ğŸ”‘ [CACHE] Generated cache key: {cache_key}")
            
            # ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬
            logger.info(f"âœ… [CACHE] Checking cache validity for key: {cache_key}")
            if not self.is_cache_valid(cache_key):
                logger.info(f"âŒ [CACHE] Cache not valid for key: {cache_key}")
                return None
            
            logger.info(f"âœ… [CACHE] Cache is valid for key: {cache_key}")
            
            cache_file = self._get_cache_file_path(cache_key)
            logger.info(f"ğŸ“ [CACHE] Cache file path: {cache_file}")
            
            if not os.path.exists(cache_file):
                # ë©”íƒ€ë°ì´í„°ëŠ” ìˆì§€ë§Œ íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°
                logger.warning(f"âŒ [CACHE] Cache metadata exists but file missing: {cache_key}")
                logger.warning(f"ğŸ“ [CACHE] Missing file: {cache_file}")
                self._remove_cache_entry(cache_key)
                return None
            
            logger.info(f"âœ… [CACHE] Cache file exists: {cache_file}")
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = os.path.getsize(cache_file)
            logger.info(f"ğŸ“Š [CACHE] Cache file size: {file_size} bytes")
            
            if file_size == 0:
                logger.warning(f"âš ï¸ [CACHE] Cache file is empty: {cache_file}")
                self._remove_cache_entry(cache_key)
                return None
            
            try:
                logger.info(f"ğŸ“– [CACHE] Loading cache file: {cache_file}")
                with open(cache_file, 'rb') as f:
                    data = pickle.load(f)
                
                logger.info(f"âœ… [CACHE] Successfully loaded cache data: {type(data)}")
                
                # ì•¡ì„¸ìŠ¤ ì‹œê°„ ì—…ë°ì´íŠ¸
                logger.info(f"ğŸ• [CACHE] Updating access time for key: {cache_key}")
                self.metadata["entries"][cache_key]["last_accessed"] = datetime.now().isoformat()
                self._save_metadata()
                
                logger.info(f"ğŸ“¦ [CACHE] Cache hit: {corp_code} {year}Q{quarter} ({data_type})")
                return data
                
            except pickle.UnpicklingError as e:
                logger.error(f"âŒ [CACHE] Pickle unpickling error: {e}")
                logger.error(f"ğŸ“ [CACHE] Corrupted cache file: {cache_file}")
                self._remove_cache_entry(cache_key)
                return None
            except EOFError as e:
                logger.error(f"âŒ [CACHE] EOF error reading cache file: {e}")
                logger.error(f"ğŸ“ [CACHE] Incomplete cache file: {cache_file}")
                self._remove_cache_entry(cache_key)
                return None
            except Exception as e:
                logger.error(f"âŒ [CACHE] Failed to load cached data: {e}")
                logger.error(f"ğŸ“ [CACHE] Error reading cache file: {cache_file}")
                import traceback
                logger.error(f"âŒ [CACHE] Traceback: {traceback.format_exc()}")
                self._remove_cache_entry(cache_key)
                return None
                
        except Exception as e:
            logger.error(f"âŒ [CACHE] get_cached_data failed: {e}")
            import traceback
            logger.error(f"âŒ [CACHE] Traceback: {traceback.format_exc()}")
            return None
    
    def cache_data(self, corp_code: str, year: int, quarter: int, data: Any, 
                   data_type: str = "financial", company_name: str = "") -> bool:
        """ë°ì´í„° ìºì‹œ ì €ì¥"""
        if data is None:
            logger.warning(f"Empty data provided for caching: {corp_code} {year}Q{quarter}")
            return False
        
        # DataFrameì¸ ê²½ìš°ì—ë§Œ empty ì²´í¬
        if hasattr(data, 'empty') and data.empty:
            logger.warning(f"Empty DataFrame provided for caching: {corp_code} {year}Q{quarter}")
            return False
        
        cache_key = self._generate_cache_key(corp_code, year, quarter, data_type)
        cache_file = self._get_cache_file_path(cache_key)
        
        try:
            # ë°ì´í„° ì €ì¥
            with open(cache_file, 'wb') as f:
                pickle.dump(data, f)
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            now = datetime.now().isoformat()
            self.metadata["entries"][cache_key] = {
                "corp_code": corp_code,
                "company_name": company_name,
                "year": year,
                "quarter": quarter,
                "data_type": data_type,
                "cached_at": now,
                "last_accessed": now,
                "file_size": os.path.getsize(cache_file),
                "record_count": len(data) if hasattr(data, '__len__') else 1
            }
            
            self.metadata["total_entries"] = len(self.metadata["entries"])
            self._save_metadata()
            
            record_count = len(data) if hasattr(data, '__len__') else 1
            logger.info(f"ğŸ’¾ Cached: {corp_code} {year}Q{quarter} ({data_type}) - {record_count} records")
            return True
            
        except Exception as e:
            logger.error(f"Failed to cache data: {e}")
            # ì‹¤íŒ¨í•œ ê²½ìš° íŒŒì¼ ì •ë¦¬
            if os.path.exists(cache_file):
                os.remove(cache_file)
            return False
    
    def _remove_cache_entry(self, cache_key: str):
        """ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±°"""
        cache_file = self._get_cache_file_path(cache_key)
        
        # íŒŒì¼ ì‚­ì œ
        if os.path.exists(cache_file):
            os.remove(cache_file)
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì œê±°
        if cache_key in self.metadata["entries"]:
            del self.metadata["entries"][cache_key]
            self.metadata["total_entries"] = len(self.metadata["entries"])
            self._save_metadata()
    
    def invalidate_cache(self, corp_code: str = None, year: int = None, quarter: int = None) -> int:
        """ìºì‹œ ë¬´íš¨í™”"""
        removed_count = 0
        keys_to_remove = []
        
        for cache_key, entry in self.metadata["entries"].items():
            should_remove = True
            
            if corp_code and entry["corp_code"] != corp_code:
                should_remove = False
            if year and entry["year"] != year:
                should_remove = False
            if quarter and entry["quarter"] != quarter:
                should_remove = False
            
            if should_remove:
                keys_to_remove.append(cache_key)
        
        for cache_key in keys_to_remove:
            self._remove_cache_entry(cache_key)
            removed_count += 1
        
        if removed_count > 0:
            logger.info(f"ğŸ—‘ï¸ Invalidated {removed_count} cache entries")
        
        return removed_count
    
    def cleanup_expired_cache(self) -> int:
        """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
        removed_count = 0
        keys_to_remove = []
        
        for cache_key, entry in self.metadata["entries"].items():
            cached_time = datetime.fromisoformat(entry["cached_at"])
            if datetime.now() - cached_time >= self.cache_duration:
                keys_to_remove.append(cache_key)
        
        for cache_key in keys_to_remove:
            self._remove_cache_entry(cache_key)
            removed_count += 1
        
        if removed_count > 0:
            logger.info(f"ğŸ§¹ Cleaned up {removed_count} expired cache entries")
        
        return removed_count
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì •ë³´"""
        total_size = 0
        valid_entries = 0
        expired_entries = 0
        companies = set()
        
        for cache_key, entry in self.metadata["entries"].items():
            total_size += entry.get("file_size", 0)
            companies.add(entry["company_name"])
            
            if self.is_cache_valid(cache_key):
                valid_entries += 1
            else:
                expired_entries += 1
        
        return {
            "total_entries": len(self.metadata["entries"]),
            "valid_entries": valid_entries,
            "expired_entries": expired_entries,
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "companies_cached": len(companies),
            "cache_duration_hours": self.cache_duration.total_seconds() / 3600,
            "cache_created": self.metadata.get("cache_created", "Unknown")
        }
    
    def list_cached_entries(self, corp_code: str = None) -> List[Dict[str, Any]]:
        """ìºì‹œëœ ì—”íŠ¸ë¦¬ ëª©ë¡ ì¡°íšŒ"""
        entries = []
        
        for cache_key, entry in self.metadata["entries"].items():
            if corp_code and entry["corp_code"] != corp_code:
                continue
            
            entry_info = entry.copy()
            entry_info["cache_key"] = cache_key
            entry_info["is_valid"] = self.is_cache_valid(cache_key)
            entry_info["file_exists"] = os.path.exists(self._get_cache_file_path(cache_key))
            
            entries.append(entry_info)
        
        # ìµœì‹  ìˆœìœ¼ë¡œ ì •ë ¬
        entries.sort(key=lambda x: x["cached_at"], reverse=True)
        return entries
    
    def clear_all_cache(self) -> int:
        """ëª¨ë“  ìºì‹œ ì‚­ì œ"""
        removed_count = len(self.metadata["entries"])
        
        # ëª¨ë“  ìºì‹œ íŒŒì¼ ì‚­ì œ
        try:
            for cache_key in list(self.metadata["entries"].keys()):
                self._remove_cache_entry(cache_key)
            
            logger.info(f"ğŸ—‘ï¸ Cleared all cache ({removed_count} entries)")
            return removed_count
            
        except Exception as e:
            logger.error(f"Error clearing cache: {e}")
            return 0


def create_default_cache() -> DARTDataCache:
    """ê¸°ë³¸ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return DARTDataCache(
        cache_dir="financial_data/dart_cache",
        cache_duration_hours=24  # 24ì‹œê°„ ìºì‹œ
    )


# ê¸€ë¡œë²Œ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ (í•„ìš”ì‹œ ì‚¬ìš©)
_global_cache = None

def get_global_cache() -> DARTDataCache:
    """ê¸€ë¡œë²Œ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_cache
    if _global_cache is None:
        _global_cache = create_default_cache()
    return _global_cache