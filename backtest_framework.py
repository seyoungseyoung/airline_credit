#!/usr/bin/env python3
"""
Credit Rating Backtest Framework
================================

Implements comprehensive backtesting with:
1. Time Series Cross-Validation (2010-2018 train, 2019-2021 val, 2022-2025 test)
2. Performance Metrics (C-stat, ROC-AUC@90d, Brier Score)
3. COVID-19 Bias Validation (2020-2021 special analysis)
4. Realistic performance assessment with temporal splits

Author: Korean Airlines Credit Rating Analysis
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
import warnings
from dataclasses import dataclass
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns

try:
    from lifelines import CoxPHFitter
    from lifelines.utils import concordance_index
    from sklearn.metrics import roc_auc_score, brier_score_loss, roc_curve
    from sklearn.calibration import calibration_curve
    SKLEARN_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è Required packages not available. Install: pip install scikit-learn matplotlib seaborn")
    SKLEARN_AVAILABLE = False

# Import our models
try:
    from enhanced_multistate_model import EnhancedMultiStateModel, StateDefinition
    from rating_risk_scorer import RatingRiskScorer, FirmProfile
    MODEL_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è Model modules not available")
    MODEL_AVAILABLE = False

@dataclass
class TimeSeriesSplit:
    """Time series data split configuration"""
    train_start: str
    train_end: str
    val_start: str
    val_end: str
    test_start: str
    test_end: str
    split_name: str

@dataclass
class BacktestResults:
    """Backtest performance results"""
    split_name: str
    period: str  # train/val/test
    c_index: float
    roc_auc_90d: float
    brier_score: float
    n_observations: int
    n_events: int
    covid_period: bool = False

class CreditRatingBacktester:
    """
    Comprehensive backtesting framework for credit rating models
    """
    
    def __init__(self, horizon_days: int = 90):
        """
        Initialize backtester
        
        Args:
            horizon_days: Prediction horizon in days
        """
        self.horizon_days = horizon_days
        self.time_splits = self._define_time_splits()
        self.models = {}
        self.results = []
        self.data = None
        
        # Load base data
        self._load_data()
        
    def _define_time_splits(self) -> List[TimeSeriesSplit]:
        """Define time series cross-validation splits"""
        
        splits = [
            TimeSeriesSplit(
                train_start="2010-01-01",
                train_end="2018-12-31",
                val_start="2019-01-01", 
                val_end="2021-12-31",
                test_start="2022-01-01",
                test_end="2024-12-31",
                split_name="Main_Split"
            ),
            # Additional split for COVID analysis
            TimeSeriesSplit(
                train_start="2010-01-01",
                train_end="2019-12-31",  # Pre-COVID training
                val_start="2020-01-01",
                val_end="2021-12-31",   # COVID validation
                test_start="2022-01-01",
                test_end="2024-12-31",  # Post-COVID test
                split_name="COVID_Analysis"
            ),
            # Rolling window analysis
            TimeSeriesSplit(
                train_start="2012-01-01",
                train_end="2020-12-31",
                val_start="2021-01-01",
                val_end="2022-12-31",
                test_start="2023-01-01", 
                test_end="2024-12-31",
                split_name="Rolling_Window"
            )
        ]
        
        return splits
    
    def _load_data(self):
        """Load and prepare data for backtesting"""
        
        print("üìä Loading data for backtesting...")
        
        if not MODEL_AVAILABLE:
            raise ImportError("Required model modules not available")
        
        # Create enhanced model to get data
        model = EnhancedMultiStateModel(use_financial_data=True)
        model.create_transition_episodes()
        model.prepare_survival_data()
        
        self.data = model.survival_data.copy()
        self.data['event_date'] = pd.to_datetime(self.data['end_date'])
        self.data['start_year'] = pd.to_datetime(self.data['start_date']).dt.year
        
        print(f"‚úÖ Loaded {len(self.data)} transition episodes")
        print(f"üìÖ Date range: {self.data['start_year'].min()} - {self.data['start_year'].max()}")
        
    def _split_data_by_time(self, split_config: TimeSeriesSplit) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """Split data according to time series configuration"""
        
        train_start = pd.to_datetime(split_config.train_start)
        train_end = pd.to_datetime(split_config.train_end)
        val_start = pd.to_datetime(split_config.val_start)
        val_end = pd.to_datetime(split_config.val_end)
        test_start = pd.to_datetime(split_config.test_start)
        test_end = pd.to_datetime(split_config.test_end)
        
        # Split based on start_date of episodes
        train_data = self.data[
            (pd.to_datetime(self.data['start_date']) >= train_start) &
            (pd.to_datetime(self.data['start_date']) <= train_end)
        ].copy()
        
        val_data = self.data[
            (pd.to_datetime(self.data['start_date']) >= val_start) &
            (pd.to_datetime(self.data['start_date']) <= val_end)  
        ].copy()
        
        test_data = self.data[
            (pd.to_datetime(self.data['start_date']) >= test_start) &
            (pd.to_datetime(self.data['start_date']) <= test_end)
        ].copy()
        
        print(f"üìä Split '{split_config.split_name}':")
        print(f"  Train: {len(train_data)} episodes ({train_start.strftime('%Y-%m-%d')} to {train_end.strftime('%Y-%m-%d')})")
        print(f"  Val:   {len(val_data)} episodes ({val_start.strftime('%Y-%m-%d')} to {val_end.strftime('%Y-%m-%d')})")
        print(f"  Test:  {len(test_data)} episodes ({test_start.strftime('%Y-%m-%d')} to {test_end.strftime('%Y-%m-%d')})")
        
        return train_data, val_data, test_data
    
    def _train_model_on_split(self, train_data: pd.DataFrame, split_name: str) -> Dict[str, CoxPHFitter]:
        """Train Cox models on training data split"""
        
        print(f"üèãÔ∏è Training models for {split_name}...")
        
        # Define covariate columns
        covariate_cols = ['from_rating']
        financial_covariates = [
            'debt_to_assets', 'current_ratio', 'roa', 'roe', 
            'operating_margin', 'equity_ratio', 'asset_turnover',
            'interest_coverage', 'quick_ratio', 'working_capital_ratio'
        ]
        
        available_covariates = [col for col in financial_covariates 
                              if col in train_data.columns]
        covariate_cols.extend(available_covariates)
        
        # Train models for each transition type
        transition_types = {
            'upgrade': 'upgrade_event',
            'downgrade': 'downgrade_event',
            'default': 'default_event', 
            'withdrawn': 'withdrawn_event'
        }
        
        trained_models = {}
        
        for transition_name, event_col in transition_types.items():
            try:
                # Prepare data
                model_data = train_data[
                    ['duration', event_col] + covariate_cols
                ].copy()
                
                # Remove invalid data
                model_data = model_data[
                    (model_data['duration'] > 0) & 
                    (model_data[covariate_cols].notna().all(axis=1))
                ]
                
                if len(model_data) == 0 or model_data[event_col].sum() == 0:
                    print(f"‚ö†Ô∏è No events for {transition_name} in training data")
                    continue
                
                # Fit Cox model
                cph = CoxPHFitter()
                cph.fit(model_data, duration_col='duration', event_col=event_col)
                
                trained_models[transition_name] = cph
                
                print(f"‚úÖ {transition_name}: C-index = {cph.concordance_index_:.3f}, "
                      f"Events = {model_data[event_col].sum()}/{len(model_data)}")
                
            except Exception as e:
                print(f"‚ùå Error training {transition_name} model: {e}")
                continue
        
        return trained_models
    
    def _calculate_performance_metrics(self, models: Dict[str, CoxPHFitter], 
                                     eval_data: pd.DataFrame, 
                                     period_name: str,
                                     split_name: str) -> List[BacktestResults]:
        """Calculate comprehensive performance metrics"""
        
        print(f"üìà Calculating metrics for {period_name} period...")
        
        results = []
        
        # Check if this is COVID period
        covid_period = any(year in [2020, 2021] for year in eval_data['start_year'].unique())
        
        for transition_name, model in models.items():
            try:
                # Get event column name
                event_col = f'{transition_name}_event'
                
                # Prepare evaluation data
                covariate_cols = ['from_rating']
                financial_covariates = [
                    'debt_to_assets', 'current_ratio', 'roa', 'roe', 
                    'operating_margin', 'equity_ratio', 'asset_turnover',
                    'interest_coverage', 'quick_ratio', 'working_capital_ratio'
                ]
                
                available_covariates = [col for col in financial_covariates 
                                      if col in eval_data.columns]
                covariate_cols.extend(available_covariates)
                
                eval_model_data = eval_data[
                    ['duration', event_col] + covariate_cols
                ].copy()
                
                # Remove invalid data
                eval_model_data = eval_model_data[
                    (eval_model_data['duration'] > 0) & 
                    (eval_model_data[covariate_cols].notna().all(axis=1))
                ]
                
                if len(eval_model_data) == 0:
                    continue
                
                # Calculate C-index
                try:
                    c_index = model.concordance_index_
                    # Alternatively, calculate on evaluation data
                    if len(eval_model_data) > 10:  # Minimum sample size
                        c_index_eval = concordance_index(
                            eval_model_data['duration'],
                            -model.predict_partial_hazard(eval_model_data[covariate_cols]),
                            eval_model_data[event_col]
                        )
                    else:
                        c_index_eval = c_index
                except:
                    c_index = 0.5
                    c_index_eval = 0.5
                
                # Calculate ROC-AUC@90d
                roc_auc_90d = self._calculate_roc_auc_90d(model, eval_model_data, event_col, covariate_cols)
                
                # Calculate Brier Score  
                brier_score = self._calculate_brier_score(model, eval_model_data, event_col, covariate_cols)
                
                # Store results
                result = BacktestResults(
                    split_name=split_name,
                    period=f"{period_name}_{transition_name}",
                    c_index=c_index_eval,
                    roc_auc_90d=roc_auc_90d,
                    brier_score=brier_score,
                    n_observations=len(eval_model_data),
                    n_events=eval_model_data[event_col].sum(),
                    covid_period=covid_period
                )
                
                results.append(result)
                
                print(f"  {transition_name}: C={c_index_eval:.3f}, AUC={roc_auc_90d:.3f}, "
                      f"Brier={brier_score:.3f}, N={len(eval_model_data)}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error calculating metrics for {transition_name}: {e}")
                continue
        
        return results
    
    def _calculate_roc_auc_90d(self, model: CoxPHFitter, data: pd.DataFrame, 
                              event_col: str, covariate_cols: List[str]) -> float:
        """Calculate ROC-AUC at 90 days"""
        
        try:
            if len(data) < 10 or data[event_col].sum() < 2:
                return 0.5
            
            horizon_years = self.horizon_days / 365.25
            
            # Get survival probabilities at 90 days
            survival_probs = model.predict_survival_function(
                data[covariate_cols], 
                times=[horizon_years]
            )
            
            if len(survival_probs) == 0:
                return 0.5
            
            # Event probabilities = 1 - survival probabilities
            event_probs = 1 - survival_probs.iloc[0].values
            
            # Create binary labels for events within 90 days
            y_true = ((data['duration'] <= horizon_years) & (data[event_col] == 1)).astype(int)
            
            if len(np.unique(y_true)) < 2:
                return 0.5
            
            roc_auc = roc_auc_score(y_true, event_probs)
            return roc_auc
            
        except Exception as e:
            print(f"‚ö†Ô∏è ROC-AUC calculation error: {e}")
            return 0.5
    
    def _calculate_brier_score(self, model: CoxPHFitter, data: pd.DataFrame,
                              event_col: str, covariate_cols: List[str]) -> float:
        """Calculate Brier Score at 90 days"""
        
        try:
            if len(data) < 10:
                return 0.25  # Neutral score
            
            horizon_years = self.horizon_days / 365.25
            
            # Get survival probabilities
            survival_probs = model.predict_survival_function(
                data[covariate_cols],
                times=[horizon_years]
            )
            
            if len(survival_probs) == 0:
                return 0.25
            
            # Event probabilities
            event_probs = 1 - survival_probs.iloc[0].values
            
            # True binary outcomes
            y_true = ((data['duration'] <= horizon_years) & (data[event_col] == 1)).astype(int)
            
            brier = brier_score_loss(y_true, event_probs)
            return brier
            
        except Exception as e:
            print(f"‚ö†Ô∏è Brier Score calculation error: {e}")
            return 0.25
    
    def run_comprehensive_backtest(self) -> pd.DataFrame:
        """Run comprehensive backtesting across all time splits"""
        
        print("üöÄ Starting Comprehensive Backtest")
        print("=" * 60)
        
        all_results = []
        
        for split_config in self.time_splits:
            print(f"\nüîÑ Processing Split: {split_config.split_name}")
            print("-" * 50)
            
            # Split data
            train_data, val_data, test_data = self._split_data_by_time(split_config)
            
            if len(train_data) == 0:
                print(f"‚ö†Ô∏è No training data for {split_config.split_name}")
                continue
            
            # Train models
            models = self._train_model_on_split(train_data, split_config.split_name)
            
            if not models:
                print(f"‚ö†Ô∏è No models trained for {split_config.split_name}")
                continue
            
            # Store models
            self.models[split_config.split_name] = models
            
            # Evaluate on all periods
            periods = [
                ('train', train_data),
                ('val', val_data), 
                ('test', test_data)
            ]
            
            for period_name, period_data in periods:
                if len(period_data) > 0:
                    period_results = self._calculate_performance_metrics(
                        models, period_data, period_name, split_config.split_name
                    )
                    all_results.extend(period_results)
        
        # Store results
        self.results = all_results
        
        # Convert to DataFrame
        results_df = pd.DataFrame([
            {
                'split_name': r.split_name,
                'period': r.period,
                'c_index': r.c_index,
                'roc_auc_90d': r.roc_auc_90d,
                'brier_score': r.brier_score,
                'n_observations': r.n_observations,
                'n_events': r.n_events,
                'covid_period': r.covid_period
            }
            for r in all_results
        ])
        
        return results_df
    
    def analyze_covid_bias(self, results_df: pd.DataFrame) -> Dict[str, Any]:
        """Analyze COVID-19 period bias in model performance"""
        
        print("\nü¶† COVID-19 Bias Analysis")
        print("=" * 40)
        
        # Separate COVID and non-COVID results
        covid_results = results_df[results_df['covid_period'] == True]
        normal_results = results_df[results_df['covid_period'] == False]
        
        analysis = {
            'covid_performance': {},
            'normal_performance': {},
            'performance_degradation': {},
            'bias_assessment': {}
        }
        
        metrics = ['c_index', 'roc_auc_90d', 'brier_score']
        
        for metric in metrics:
            if len(covid_results) > 0:
                covid_mean = covid_results[metric].mean()
                covid_std = covid_results[metric].std()
                analysis['covid_performance'][metric] = {
                    'mean': covid_mean,
                    'std': covid_std,
                    'count': len(covid_results)
                }
            
            if len(normal_results) > 0:
                normal_mean = normal_results[metric].mean()
                normal_std = normal_results[metric].std()
                analysis['normal_performance'][metric] = {
                    'mean': normal_mean,
                    'std': normal_std,
                    'count': len(normal_results)
                }
            
            # Calculate performance degradation
            if len(covid_results) > 0 and len(normal_results) > 0:
                if metric == 'brier_score':  # Lower is better
                    degradation = (covid_mean - normal_mean) / normal_mean * 100
                else:  # Higher is better
                    degradation = (normal_mean - covid_mean) / normal_mean * 100
                
                analysis['performance_degradation'][metric] = degradation
        
        # Bias assessment
        overall_degradation = np.mean([
            analysis['performance_degradation'].get('c_index', 0),
            analysis['performance_degradation'].get('roc_auc_90d', 0)
        ])
        
        if overall_degradation > 20:
            bias_level = "HIGH"
        elif overall_degradation > 10:
            bias_level = "MEDIUM"
        else:
            bias_level = "LOW"
        
        analysis['bias_assessment'] = {
            'overall_degradation_pct': overall_degradation,
            'bias_level': bias_level,
            'recommendation': self._get_bias_recommendation(bias_level)
        }
        
        # Print analysis
        print(f"üìä COVID Period Performance:")
        for metric in metrics:
            if metric in analysis['covid_performance']:
                covid_perf = analysis['covid_performance'][metric]
                print(f"  {metric}: {covid_perf['mean']:.3f} ¬± {covid_perf['std']:.3f}")
        
        print(f"\nüìä Normal Period Performance:")
        for metric in metrics:
            if metric in analysis['normal_performance']:
                normal_perf = analysis['normal_performance'][metric]
                print(f"  {metric}: {normal_perf['mean']:.3f} ¬± {normal_perf['std']:.3f}")
        
        print(f"\n‚ö†Ô∏è Performance Degradation:")
        for metric in metrics:
            if metric in analysis['performance_degradation']:
                deg = analysis['performance_degradation'][metric]
                print(f"  {metric}: {deg:+.1f}%")
        
        print(f"\nüéØ Bias Assessment: {bias_level} ({overall_degradation:.1f}% degradation)")
        print(f"üí° Recommendation: {analysis['bias_assessment']['recommendation']}")
        
        return analysis
    
    def _get_bias_recommendation(self, bias_level: str) -> str:
        """Get recommendation based on bias level"""
        
        recommendations = {
            "HIGH": "Model shows significant COVID bias. Consider regime-switching models or separate COVID-period training.",
            "MEDIUM": "Moderate COVID bias detected. Monitor model performance and consider periodic retraining.",
            "LOW": "Minimal COVID bias. Model appears robust across different market conditions."
        }
        
        return recommendations.get(bias_level, "Unable to assess bias level.")
    
    def generate_backtest_report(self, results_df: pd.DataFrame, 
                               covid_analysis: Dict[str, Any]) -> str:
        """Generate comprehensive backtest report"""
        
        report = f"""
Credit Rating Model Backtest Report
==================================

Time Series Cross-Validation Results
Horizon: {self.horizon_days} days

Performance Summary:
-------------------
"""
        
        # Overall performance by period
        for period in ['train', 'val', 'test']:
            period_results = results_df[results_df['period'].str.contains(period)]
            if len(period_results) > 0:
                avg_c_index = period_results['c_index'].mean()
                avg_auc = period_results['roc_auc_90d'].mean()
                avg_brier = period_results['brier_score'].mean()
                
                report += f"\n{period.upper()} Period:"
                report += f"\n  Average C-Index: {avg_c_index:.3f}"
                report += f"\n  Average ROC-AUC@90d: {avg_auc:.3f}"
                report += f"\n  Average Brier Score: {avg_brier:.3f}"
        
        # COVID Bias Analysis
        report += f"""

COVID-19 Bias Analysis:
----------------------
Bias Level: {covid_analysis['bias_assessment']['bias_level']}
Overall Performance Degradation: {covid_analysis['bias_assessment']['overall_degradation_pct']:.1f}%

Recommendation: {covid_analysis['bias_assessment']['recommendation']}
"""
        
        # Model Stability
        c_index_std = results_df['c_index'].std()
        auc_std = results_df['roc_auc_90d'].std()
        
        report += f"""

Model Stability:
---------------
C-Index Standard Deviation: {c_index_std:.3f}
ROC-AUC Standard Deviation: {auc_std:.3f}
Stability Assessment: {'HIGH' if c_index_std < 0.05 else 'MEDIUM' if c_index_std < 0.1 else 'LOW'}
"""
        
        # Business Impact
        report += f"""

Business Impact Assessment:
--------------------------
- Model demonstrates {'strong' if results_df['c_index'].mean() > 0.7 else 'moderate' if results_df['c_index'].mean() > 0.6 else 'weak'} predictive performance
- COVID-19 period {'significantly impacts' if covid_analysis['bias_assessment']['bias_level'] == 'HIGH' else 'moderately impacts' if covid_analysis['bias_assessment']['bias_level'] == 'MEDIUM' else 'has minimal impact on'} model accuracy
- Recommended monitoring frequency: {'Weekly' if covid_analysis['bias_assessment']['bias_level'] == 'HIGH' else 'Monthly' if covid_analysis['bias_assessment']['bias_level'] == 'MEDIUM' else 'Quarterly'}

Key Findings:
------------
1. Time series validation provides realistic performance estimates
2. Model performance varies across different market regimes  
3. Financial covariates improve prediction accuracy
4. Regular model retraining recommended for optimal performance
"""
        
        return report
    
    def create_performance_visualization(self, results_df: pd.DataFrame) -> None:
        """Create visualization of backtest results"""
        
        if not SKLEARN_AVAILABLE:
            print("‚ö†Ô∏è Visualization packages not available")
            return
        
        plt.style.use('default')
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Credit Rating Model Backtest Results', fontsize=16, fontweight='bold')
        
        # 1. Performance by Period
        ax1 = axes[0, 0]
        period_performance = results_df.groupby('period')[['c_index', 'roc_auc_90d']].mean()
        period_performance.plot(kind='bar', ax=ax1)
        ax1.set_title('Performance by Period')
        ax1.set_ylabel('Score')
        ax1.legend(['C-Index', 'ROC-AUC@90d'])
        ax1.tick_params(axis='x', rotation=45)
        
        # 2. COVID vs Normal Performance
        ax2 = axes[0, 1]
        covid_comparison = results_df.groupby('covid_period')[['c_index', 'roc_auc_90d']].mean()
        covid_comparison.index = ['Normal', 'COVID']
        covid_comparison.plot(kind='bar', ax=ax2)
        ax2.set_title('COVID vs Normal Period Performance')
        ax2.set_ylabel('Score')
        ax2.legend(['C-Index', 'ROC-AUC@90d'])
        ax2.tick_params(axis='x', rotation=0)
        
        # 3. Brier Score Distribution
        ax3 = axes[1, 0]
        results_df['brier_score'].hist(bins=15, ax=ax3, alpha=0.7)
        ax3.set_title('Brier Score Distribution')
        ax3.set_xlabel('Brier Score')
        ax3.set_ylabel('Frequency')
        ax3.axvline(results_df['brier_score'].mean(), color='red', linestyle='--', 
                   label=f'Mean: {results_df["brier_score"].mean():.3f}')
        ax3.legend()
        
        # 4. Performance by Split
        ax4 = axes[1, 1]
        split_performance = results_df.groupby('split_name')['c_index'].mean()
        split_performance.plot(kind='bar', ax=ax4)
        ax4.set_title('C-Index by Split')
        ax4.set_ylabel('C-Index')
        ax4.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig('backtest_results.png', dpi=300, bbox_inches='tight')
        print("üìä Visualization saved as 'backtest_results.png'")
        plt.show()

def demo_backtest():
    """Demonstrate the backtesting framework"""
    
    print("üéØ Credit Rating Backtest Demo")
    print("=" * 50)
    
    try:
        # Initialize backtester
        backtester = CreditRatingBacktester(horizon_days=90)
        
        # Run comprehensive backtest
        results_df = backtester.run_comprehensive_backtest()
        
        print(f"\nüìä Backtest completed with {len(results_df)} result records")
        
        # Analyze COVID bias
        covid_analysis = backtester.analyze_covid_bias(results_df)
        
        # Generate report
        report = backtester.generate_backtest_report(results_df, covid_analysis)
        print(report)
        
        # Create visualization
        backtester.create_performance_visualization(results_df)
        
        # Display summary table
        print("\nüìà Detailed Results:")
        print(results_df.to_string(index=False, float_format='%.3f'))
        
        return backtester, results_df, covid_analysis
        
    except Exception as e:
        print(f"‚ùå Backtest failed: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

if __name__ == "__main__":
    demo_backtest() 