#!/usr/bin/env python3
"""
DART ë°ì´í„° ìºì‹œ ì‹œìŠ¤í…œ
===================

ì‹¤ì œ DART API í˜¸ì¶œ ê²°ê³¼ë¥¼ ìºì‹œí•˜ì—¬ ì¤‘ë³µ ìˆ˜ì§‘ì„ ë°©ì§€í•˜ëŠ” ì‹œìŠ¤í…œ

Author: Korean Airlines Credit Rating Analysis
"""

import os
import json
import pickle
import hashlib
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List
import pandas as pd
import logging

logger = logging.getLogger(__name__)

class DARTDataCache:
    """
    DART API ë°ì´í„° ìºì‹œ ê´€ë¦¬ ì‹œìŠ¤í…œ
    """
    
    def __init__(self, cache_dir: str = "dart_cache", cache_duration_hours: int = 24):
        """
        ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            cache_dir: ìºì‹œ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
            cache_duration_hours: ìºì‹œ ìœ íš¨ ì‹œê°„ (ì‹œê°„ ë‹¨ìœ„)
        """
        self.cache_dir = cache_dir
        self.cache_duration = timedelta(hours=cache_duration_hours)
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        self.metadata_file = os.path.join(self.cache_dir, "cache_metadata.json")
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ ë˜ëŠ” ì´ˆê¸°í™”
        self.metadata = self._load_metadata()
        
        logger.info(f"âœ… DART Cache initialized: {self.cache_dir}")
        logger.info(f"ğŸ• Cache duration: {cache_duration_hours} hours")
    
    def _load_metadata(self) -> Dict[str, Any]:
        """ìºì‹œ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        if os.path.exists(self.metadata_file):
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Failed to load cache metadata: {e}")
        
        return {
            "cache_created": datetime.now().isoformat(),
            "total_entries": 0,
            "entries": {}
        }
    
    def _save_metadata(self):
        """ìºì‹œ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"Failed to save cache metadata: {e}")
    
    def _generate_cache_key(self, corp_code: str, year: int, quarter: int, data_type: str = "financial") -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_string = f"{corp_code}_{year}_{quarter}_{data_type}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _get_cache_file_path(self, cache_key: str) -> str:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
        return os.path.join(self.cache_dir, f"{cache_key}.pkl")
    
    def is_cache_valid(self, cache_key: str) -> bool:
        """ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬"""
        if cache_key not in self.metadata["entries"]:
            return False
        
        entry = self.metadata["entries"][cache_key]
        cached_time = datetime.fromisoformat(entry["cached_at"])
        
        return datetime.now() - cached_time < self.cache_duration
    
    def get_cached_data(self, corp_code: str, year: int, quarter: int, data_type: str = "financial") -> Optional[pd.DataFrame]:
        """ìºì‹œëœ ë°ì´í„° ì¡°íšŒ"""
        cache_key = self._generate_cache_key(corp_code, year, quarter, data_type)
        
        if not self.is_cache_valid(cache_key):
            return None
        
        cache_file = self._get_cache_file_path(cache_key)
        
        if not os.path.exists(cache_file):
            # ë©”íƒ€ë°ì´í„°ëŠ” ìˆì§€ë§Œ íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°
            logger.warning(f"Cache metadata exists but file missing: {cache_key}")
            self._remove_cache_entry(cache_key)
            return None
        
        try:
            with open(cache_file, 'rb') as f:
                data = pickle.load(f)
            
            # ì•¡ì„¸ìŠ¤ ì‹œê°„ ì—…ë°ì´íŠ¸
            self.metadata["entries"][cache_key]["last_accessed"] = datetime.now().isoformat()
            self._save_metadata()
            
            logger.info(f"ğŸ“¦ Cache hit: {corp_code} {year}Q{quarter} ({data_type})")
            return data
            
        except Exception as e:
            logger.error(f"Failed to load cached data: {e}")
            self._remove_cache_entry(cache_key)
            return None
    
    def cache_data(self, corp_code: str, year: int, quarter: int, data: pd.DataFrame, 
                   data_type: str = "financial", company_name: str = "") -> bool:
        """ë°ì´í„° ìºì‹œ ì €ì¥"""
        if data is None or data.empty:
            logger.warning(f"Empty data provided for caching: {corp_code} {year}Q{quarter}")
            return False
        
        cache_key = self._generate_cache_key(corp_code, year, quarter, data_type)
        cache_file = self._get_cache_file_path(cache_key)
        
        try:
            # ë°ì´í„° ì €ì¥
            with open(cache_file, 'wb') as f:
                pickle.dump(data, f)
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            now = datetime.now().isoformat()
            self.metadata["entries"][cache_key] = {
                "corp_code": corp_code,
                "company_name": company_name,
                "year": year,
                "quarter": quarter,
                "data_type": data_type,
                "cached_at": now,
                "last_accessed": now,
                "file_size": os.path.getsize(cache_file),
                "record_count": len(data)
            }
            
            self.metadata["total_entries"] = len(self.metadata["entries"])
            self._save_metadata()
            
            logger.info(f"ğŸ’¾ Cached: {corp_code} {year}Q{quarter} ({data_type}) - {len(data)} records")
            return True
            
        except Exception as e:
            logger.error(f"Failed to cache data: {e}")
            # ì‹¤íŒ¨í•œ ê²½ìš° íŒŒì¼ ì •ë¦¬
            if os.path.exists(cache_file):
                os.remove(cache_file)
            return False
    
    def _remove_cache_entry(self, cache_key: str):
        """ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±°"""
        cache_file = self._get_cache_file_path(cache_key)
        
        # íŒŒì¼ ì‚­ì œ
        if os.path.exists(cache_file):
            os.remove(cache_file)
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì œê±°
        if cache_key in self.metadata["entries"]:
            del self.metadata["entries"][cache_key]
            self.metadata["total_entries"] = len(self.metadata["entries"])
            self._save_metadata()
    
    def invalidate_cache(self, corp_code: str = None, year: int = None, quarter: int = None) -> int:
        """ìºì‹œ ë¬´íš¨í™”"""
        removed_count = 0
        keys_to_remove = []
        
        for cache_key, entry in self.metadata["entries"].items():
            should_remove = True
            
            if corp_code and entry["corp_code"] != corp_code:
                should_remove = False
            if year and entry["year"] != year:
                should_remove = False
            if quarter and entry["quarter"] != quarter:
                should_remove = False
            
            if should_remove:
                keys_to_remove.append(cache_key)
        
        for cache_key in keys_to_remove:
            self._remove_cache_entry(cache_key)
            removed_count += 1
        
        if removed_count > 0:
            logger.info(f"ğŸ—‘ï¸ Invalidated {removed_count} cache entries")
        
        return removed_count
    
    def cleanup_expired_cache(self) -> int:
        """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
        removed_count = 0
        keys_to_remove = []
        
        for cache_key, entry in self.metadata["entries"].items():
            cached_time = datetime.fromisoformat(entry["cached_at"])
            if datetime.now() - cached_time >= self.cache_duration:
                keys_to_remove.append(cache_key)
        
        for cache_key in keys_to_remove:
            self._remove_cache_entry(cache_key)
            removed_count += 1
        
        if removed_count > 0:
            logger.info(f"ğŸ§¹ Cleaned up {removed_count} expired cache entries")
        
        return removed_count
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì •ë³´"""
        total_size = 0
        valid_entries = 0
        expired_entries = 0
        companies = set()
        
        for cache_key, entry in self.metadata["entries"].items():
            total_size += entry.get("file_size", 0)
            companies.add(entry["company_name"])
            
            if self.is_cache_valid(cache_key):
                valid_entries += 1
            else:
                expired_entries += 1
        
        return {
            "total_entries": len(self.metadata["entries"]),
            "valid_entries": valid_entries,
            "expired_entries": expired_entries,
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "companies_cached": len(companies),
            "cache_duration_hours": self.cache_duration.total_seconds() / 3600,
            "cache_created": self.metadata.get("cache_created", "Unknown")
        }
    
    def list_cached_entries(self, corp_code: str = None) -> List[Dict[str, Any]]:
        """ìºì‹œëœ ì—”íŠ¸ë¦¬ ëª©ë¡ ì¡°íšŒ"""
        entries = []
        
        for cache_key, entry in self.metadata["entries"].items():
            if corp_code and entry["corp_code"] != corp_code:
                continue
            
            entry_info = entry.copy()
            entry_info["cache_key"] = cache_key
            entry_info["is_valid"] = self.is_cache_valid(cache_key)
            entry_info["file_exists"] = os.path.exists(self._get_cache_file_path(cache_key))
            
            entries.append(entry_info)
        
        # ìµœì‹  ìˆœìœ¼ë¡œ ì •ë ¬
        entries.sort(key=lambda x: x["cached_at"], reverse=True)
        return entries
    
    def clear_all_cache(self) -> int:
        """ëª¨ë“  ìºì‹œ ì‚­ì œ"""
        removed_count = len(self.metadata["entries"])
        
        # ëª¨ë“  ìºì‹œ íŒŒì¼ ì‚­ì œ
        try:
            for cache_key in list(self.metadata["entries"].keys()):
                self._remove_cache_entry(cache_key)
            
            logger.info(f"ğŸ—‘ï¸ Cleared all cache ({removed_count} entries)")
            return removed_count
            
        except Exception as e:
            logger.error(f"Error clearing cache: {e}")
            return 0


def create_default_cache() -> DARTDataCache:
    """ê¸°ë³¸ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return DARTDataCache(
        cache_dir="financial_data/dart_cache",
        cache_duration_hours=24  # 24ì‹œê°„ ìºì‹œ
    )


# ê¸€ë¡œë²Œ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ (í•„ìš”ì‹œ ì‚¬ìš©)
_global_cache = None

def get_global_cache() -> DARTDataCache:
    """ê¸€ë¡œë²Œ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_cache
    if _global_cache is None:
        _global_cache = create_default_cache()
    return _global_cache